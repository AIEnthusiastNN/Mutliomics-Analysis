{"cells":[{"cell_type":"markdown","id":"b48f8a4e-faea-4589-b4b7-fbedfafaf19e","metadata":{"id":"b48f8a4e-faea-4589-b4b7-fbedfafaf19e"},"source":["# Task 2: Why are the predictions to good (/bad)?"]},{"cell_type":"markdown","id":"527419fe-8e73-4bd4-bde5-28cdcc92a861","metadata":{"id":"527419fe-8e73-4bd4-bde5-28cdcc92a861"},"source":["## Question"]},{"cell_type":"markdown","id":"1ae778c4-8eaa-4b28-91f9-58004a71f914","metadata":{"id":"1ae778c4-8eaa-4b28-91f9-58004a71f914"},"source":["> I ran the following code for a binary classification task w/ an SVM in both R (first sample) and Python (second example).\n",">\n","> Given randomly generated data (X) and response (Y), this code performs leave group out cross validation 1000 times. Each entry of Y is therefore the mean of the prediction across CV iterations.\n",">\n","> Computing area under the curve should give ~0.5, since X and Y are completely random. However, this is not what we see. Area under the curve is frequently significantly higher than 0.5. The number of rows of X is very small, which can obviously cause problems.\n",">\n","> Any idea what could be happening here? I know that I can either increase the number of rows of X or decrease the number of columns to mediate the problem, but I am looking for other issues."]},{"cell_type":"markdown","id":"7eecb359-6799-4523-9cdd-05187e1230b2","metadata":{"id":"7eecb359-6799-4523-9cdd-05187e1230b2"},"source":["```R\n","Y=as.factor(rep(c(1,2), times=14))\n","X=matrix(runif(length(Y)*100), nrow=length(Y))\n","\n","library(e1071)\n","library(pROC)\n","\n","colnames(X)=1:ncol(X)\n","iter=1000\n","ansMat=matrix(NA,length(Y),iter)\n","for(i in seq(iter)){    \n","    #get train\n","\n","    train=sample(seq(length(Y)),0.5*length(Y))\n","    if(min(table(Y[train]))==0)\n","    next\n","\n","    #test from train\n","    test=seq(length(Y))[-train]\n","\n","    #train model\n","    XX=X[train,]\n","    YY=Y[train]\n","    mod=svm(XX,YY,probability=FALSE)\n","    XXX=X[test,]\n","    predVec=predict(mod,XXX)\n","    RFans=attr(predVec,'decision.values')\n","    ansMat[test,i]=as.numeric(predVec)\n","}\n","\n","ans=rowMeans(ansMat,na.rm=TRUE)\n","\n","r=roc(Y,ans)$auc\n","print(r)\n","```"]},{"cell_type":"markdown","id":"d14e4617-608b-4749-9ec5-edf15814f5bf","metadata":{"id":"d14e4617-608b-4749-9ec5-edf15814f5bf"},"source":["Similarly, when I implement the same thing in Python I get similar results.\n","\n"]},{"cell_type":"code","execution_count":6,"id":"0cf44d8d-ca36-4d6b-bafd-78cade069751","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cf44d8d-ca36-4d6b-bafd-78cade069751","executionInfo":{"status":"ok","timestamp":1719077296750,"user_tz":420,"elapsed":1362,"user":{"displayName":"bb bb","userId":"16771392911828807663"}},"outputId":"0a518fab-c267-404d-8528-77447677fc26"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7857142857142857\n"]}],"source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","\n","Y = np.array([1, 2]*14)\n","X = np.random.uniform(size=[len(Y), 100])\n","n_iter = 1000\n","ansMat = np.full((len(Y), n_iter), np.nan)\n","for i in range(n_iter):\n","    # Get train/test index\n","    train = np.random.choice(range(len(Y)), size=int(0.5*len(Y)), replace=False, p=None)\n","    if len(np.unique(Y)) == 1:\n","        continue\n","    test = np.array([i for i in range(len(Y)) if i not in train])\n","    # train model\n","    mod = SVC(probability=False)\n","    mod.fit(X=X[train, :], y=Y[train])\n","    # predict and collect answer\n","    ansMat[test, i] = mod.predict(X[test, :])\n","ans = np.nanmean(ansMat, axis=1)\n","fpr, tpr, thresholds = roc_curve(Y, ans, pos_label=1)\n","print(auc(fpr, tpr))"]},{"cell_type":"markdown","id":"504cde2c-c5fd-4bc8-8aba-efb044d31198","metadata":{"id":"504cde2c-c5fd-4bc8-8aba-efb044d31198"},"source":["## Your answer"]},{"cell_type":"code","execution_count":null,"id":"082f817e-55be-494c-a316-93c4042ed41a","metadata":{"id":"082f817e-55be-494c-a316-93c4042ed41a"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["It sounds like you're encountering an unexpected result in the area under the curve (AUC) for your binary classification task using an SVM on randomly generated data. Given that the data is random, the AUC should indeed be close to 0.5. There are a few potential issues that could be contributing to the higher-than-expected AUC:\n","\n","**Data Imbalance in Train/Test Splits:** Even though you're sampling the train and test sets randomly, there could be imbalances in the class distributions between these sets, especially with a small number of samples. This could lead to models that perform better than random chance on certain splits.\n","\n","**Leakage from Repeated Measures:** If the dataset is too small and you are repeating the process multiple times, the random sampling might inadvertently lead to some form of leakage where the model sees similar patterns multiple times, artificially inflating performance.\n","\n","**Small Sample Size**: With a small sample size, statistical fluctuations can cause significant deviations in performance. This means that the variance in your AUC could be high simply due to the small number of samples.\n","\n","**SVM's Sensitivity to Small Sample Sizes**: SVMs can be sensitive to the size and dimensionality of the data. With high-dimensional data (many features) and few samples, SVMs might find spurious patterns that do not generalize.\n","\n","Here are a few steps to diagnose and potentially mitigate the issue:\n","\n","1. Check Class Distribution in Train/Test Splits\n","Ensure that each train/test split maintains a balanced class distribution.\n","\n","2. Use Stratified Sampling\n","Stratified sampling ensures that each train/test split has a similar class distribution.\n","\n","3. Increase Sample Size or Decrease Dimensionality\n","If possible, increase the number of samples or reduce the number of features to ensure a more stable estimation of model performance.\n","\n","4. Review Model Outputs\n","Check the predictions directly to see if they are consistent with random guessing."],"metadata":{"id":"hEqv9B5_L90o"},"id":"hEqv9B5_L90o"},{"cell_type":"code","execution_count":12,"id":"c8a25275-359a-45bd-99f5-647ea3650b95","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8a25275-359a-45bd-99f5-647ea3650b95","executionInfo":{"status":"ok","timestamp":1719077977424,"user_tz":420,"elapsed":3210,"user":{"displayName":"bb bb","userId":"16771392911828807663"}},"outputId":"69c89194-69c4-4bb8-8196-d8a712636ab1"},"outputs":[{"output_type":"stream","name":"stdout","text":["AUC: 0.2857142857142857\n"]}],"source":["import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","\n","# Generate random data\n","Y = np.array([1, 2] * 14)\n","X = np.random.uniform(size=[len(Y), 100])\n","n_iter = 1000\n","ansMat = np.full((len(Y), n_iter), np.nan)\n","\n","for i in range(n_iter):\n","    # Get train/test index ensuring stratified split\n","    train, test = train_test_split(range(len(Y)), test_size=0.5, stratify=Y)\n","\n","    # Ensure that both classes are present in the training set\n","    if len(np.unique(Y[train])) < 2:\n","        continue\n","\n","    # Train model\n","    mod = SVC(probability=False)\n","    mod.fit(X[train, :], y=Y[train])\n","\n","    # Predict and collect answer\n","    ansMat[test, i] = mod.predict(X[test, :])\n","\n","# Compute the mean predictions while ignoring NaNs\n","ans = np.nanmean(ansMat, axis=1)\n","\n","# Ensure there are no NaN values in the final predictions\n","if np.isnan(ans).any():\n","    # Fill NaN values with a neutral prediction (e.g., 0.5 in binary classification)\n","    ans = np.nan_to_num(ans, nan=0.5)\n","\n","# Calculate and print ROC AUC\n","fpr, tpr, thresholds = roc_curve(Y, ans, pos_label=1)\n","roc_auc = auc(fpr, tpr)\n","print(\"AUC:\", roc_auc)\n","\n","\n"]},{"cell_type":"markdown","id":"fcfb3ad1-3f67-403e-9ee4-775b67f76660","metadata":{"id":"fcfb3ad1-3f67-403e-9ee4-775b67f76660"},"source":["## Feedback"]},{"cell_type":"markdown","id":"867962a5-b639-4d39-882c-5a42a68e891c","metadata":{"id":"867962a5-b639-4d39-882c-5a42a68e891c"},"source":["Was this exercise is difficult or not? In either case, briefly describe why."]},{"cell_type":"code","execution_count":null,"id":"3135c16f-1f49-4d49-bb7c-ee281dcc7f36","metadata":{"id":"3135c16f-1f49-4d49-bb7c-ee281dcc7f36"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"44e24d48-7fc3-4d06-b1ca-b15e12296322","metadata":{"id":"44e24d48-7fc3-4d06-b1ca-b15e12296322"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"345378d9-436b-42d6-8444-4d0edc4b1ddb","metadata":{"id":"345378d9-436b-42d6-8444-4d0edc4b1ddb"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"nalab-milk","language":"python","name":"nalab-milk"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}